{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inamullahAI/stock-market-prediction-sentiment-analysis/blob/main/Toronto_Stock_market_Prediction_using_twitter_scraping_%26_Sentiment_Analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO6c65Lj_idz"
      },
      "source": [
        "# Install Libries  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snscrape** is a Python package that allows users to easily scrape and extract data from various social media platforms, including Twitter, Instagram, and Tumblr. It provides a simple and efficient way to retrieve data from these platforms using their respective APIs without needing to write complex web scrapers. Snscrape can be installed through pip, and once installed, it can be used to retrieve data such as tweets, user profiles, hashtags, and more, making it a powerful tool for social media analysis and research."
      ],
      "metadata": {
        "id": "sIS18L9r4FB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkVZcRZVAZlr"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libries"
      ],
      "metadata": {
        "id": "IF6oqV8hqO2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nltk:**\n",
        " The Natural Language Toolkit (NLTK) is a leading platform for building Python programs to work with human language data. It provides a suite of text processing libraries for tasks such as tokenization, stemming, tagging, parsing, semantic reasoning, and wrappers for industrial-strength NLP libraries.\n",
        "SentimentIntensityAnalyzer: It is a part of the nltk library and is used for sentiment analysis. It is a pre-trained model that uses a lexicon-based approach to score the sentiment of a given text. It returns a sentiment score ranging from -1 (negative) to +1 (positive).\n",
        "\n",
        "**snscrape: **\n",
        "snscrape is a Python package that provides an easy way to access Twitter data. It allows you to scrape Twitter content, such as tweets, profiles, and trends, without requiring any API credentials. It can retrieve both historical and real-time data.\n",
        "\n",
        "**matplotlib:**\n",
        " matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK.\n",
        "WordCloud: WordCloud is a Python library for generating word clouds, which are visual representations of text data. It is an easy-to-use package that takes a list of words and generates a word cloud image based on the frequency of each word.\n",
        "\n",
        "**seaborn:** seaborn is a data visualization library based on matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics. It includes functions for visualizing univariate and bivariate data, categorical data, and timeseries data.\n",
        "unicodedata: The unicodedata module provides access to the Unicode Character Database, which defines character properties for all Unicode characters. It is used in the code snippet to normalize the text data.\n",
        "pandas: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool. It is used in the code snippet to store and manipulate the data.\n",
        "\n",
        "**datetime:** The datetime module supplies classes for working with dates and times. It is used in the code snippet to retrieve and manipulate the date and time data.\n",
        "\n",
        "**pickle:** pickle is a Python module used for serializing and de-serializing Python object structures. It is used in the code snippet to save and load the data."
      ],
      "metadata": {
        "id": "1HWl_LgL4hoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfsBxNXJAmx3"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pickle\n",
        "import nltk\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scraping steps"
      ],
      "metadata": {
        "id": "IAtdEBHxqTrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable **query_terms** contains a list of search terms that can be used to query social media platforms, specifically Twitter, for posts related to the Toronto Stock Exchange, Shopify, the Canadian economy, business, finance, stocks, and investing. The terms include keywords and hashtags commonly used in social media discussions related to these topics. Additionally, the query includes specific Twitter handles, such as @FinancialPost and @CBCBusiness, that may provide relevant and authoritative information related to the Canadian financial landscape.\n",
        "\n"
      ],
      "metadata": {
        "id": "QQHybonz5M_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define Query accordin to your Project\n",
        "query_terms = ['TSX OR \"Toronto Stock Exchange\" OR \"TorontoStockExchange\"', 'Shopify', '#CanadaEconomy', '#CanadianBusiness', '#CanadianFinance', '#CanadaStocks', '#CanadaInvesting', '@FinancialPost', '@CBCBusiness']"
      ],
      "metadata": {
        "id": "jnhvgxSFrAPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function to extract stock and company names from tweet text, and then proceeds to scrape tweets using the **snscrape** library based on a list of query terms, a start date, and an end date. It specifies the maximum number of tweets to scrape per query and appends the date, tweet text, stock name, and company name to a list of tweets. A progress message is printed every 1000 tweets, as well as a message when all tweets have been scraped for a given query term. The resulting list of tweets is saved to a file with a filename containing the current date and time.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKZjMsZM5b29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to extract stock and company names from tweet text\n",
        "def extract_names(tweet_text):\n",
        "    stock_names = ['TSX']\n",
        "    company_names = ['Toronto Stock Exchange']\n",
        "    return stock_names, company_names\n",
        "\n",
        "# Define filename with current date and time\n",
        "now = datetime.datetime.now()\n",
        "filename = f\"tweets_{now.strftime('%Y-%m-%d_%H-%M-%S')}.pkl\"\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2023-02-28'\n",
        "num_tweets_per_query = 100000\n",
        "\n",
        "# Scrape tweets and append to list\n",
        "tweets = []\n",
        "for query_term in query_terms:\n",
        "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query_term + ' since:' + start_date + ' until:' + end_date + ' lang:en').get_items()):\n",
        "        if i >= num_tweets_per_query:\n",
        "            break\n",
        "        date = tweet.date.strftime('%Y-%m-%d')\n",
        "        text = tweet.content\n",
        "        stock_names, company_names = extract_names(text)\n",
        "        for stock_name, company_name in zip(stock_names, company_names):\n",
        "            tweets.append([date, text, stock_name, company_name])\n",
        "\n",
        "        # Print progress message every 1000 tweets\n",
        "        if len(tweets) % 1000 == 0:\n",
        "            print(f\"Scraped {len(tweets)} tweets...\")\n",
        "\n",
        "    # Print progress message for each query term\n",
        "    print(f\"Finished scraping tweets for query term: {query_term}\")"
      ],
      "metadata": {
        "id": "q9LoX98wx4sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code saves the list of scraped tweets to a file using pickle serialization. The \"wb\" flag in the \"open\" function specifies that the file should be opened in binary mode for writing. The **\"pickle.dump\"** function is used to write the tweets list to the file.\n",
        "\n"
      ],
      "metadata": {
        "id": "JNJQaBvE5oAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tweets to pickle file\n",
        "with open(filename, \"wb\") as f:\n",
        "    pickle.dump(tweets, f)"
      ],
      "metadata": {
        "id": "Tvm_WTgxx8GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading a pickle file that contains previously scraped tweets using the pickle module. The file is located in the directory and is named  Once the file is opened, the pickle.load() function is used to deserialize the data from the file and load it into the tweets variable."
      ],
      "metadata": {
        "id": "b0Uc1F7b56dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print message indicating number of tweets scraped\n",
        "print(f\"Scraped a total of {len(tweets)} tweets.\")\n",
        "with open('/content/tweets_2023-03-16_20-58-09.pkl', 'rb') as f:\n",
        "    tweets = pickle.load(f)"
      ],
      "metadata": {
        "id": "9uZ0ZRrEx-Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of tweets has been converted to a pandas DataFrame with columns 'Date', 'Tweet', 'Stock Name', and 'Company Name'. This will allow for easier data analysis and manipulation going forward."
      ],
      "metadata": {
        "id": "cxhi9NJt6OSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tweets to pandas DataFrame\n",
        "df = pd.DataFrame(tweets, columns=['Date', 'Tweet', 'Stock Name', 'Company Name'])"
      ],
      "metadata": {
        "id": "V7MlMOJOyA_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tweets to CSV file\n",
        "df.to_csv('/content/millions_tweets.csv', index=False)"
      ],
      "metadata": {
        "id": "4kYkz8FayDnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tweets_2023-03-16_20-58-09.pkl', 'rb') as f:\n",
        "    tweets = pickle.load(f)"
      ],
      "metadata": {
        "id": "iGULJkSPyGM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tweets to pandas DataFrame\n",
        "df = pd.DataFrame(tweets, columns=['Date', 'Tweet', 'Stock Name', 'Company Name'])\n",
        "\n",
        "# Save tweets to CSV file\n",
        "df.to_csv('millions_tweets.csv', index=False)"
      ],
      "metadata": {
        "id": "4yKp2-ivyIl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/millions_tweets.csv')"
      ],
      "metadata": {
        "id": "T1duyLvEpnoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx3eAqtoYrvU"
      },
      "outputs": [],
      "source": [
        "df.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "XRcAr4IzhY5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis By vader lexicon\n"
      ],
      "metadata": {
        "id": "dioNqNjjr1UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " using the **SentimentIntensityAnalyzer** class from the nltk.sentiment.vader module to analyze the sentiment of each tweet in the df DataFrame.\n",
        "\n",
        "The SentimentIntensityAnalyzer is a pre-trained model that uses a lexicon of words and their associated sentiment scores to calculate a compound score, which is a value between -1 and 1 that represents the overall sentiment of the text. A score of **-1** represents extremely **negative** **sentiment**, a score of **0** represents **neutral** **sentiment**, and a score of 1 represents extremely **positive** **sentiment**.\n",
        "\n",
        "The for loop iterates over each row in the DataFrame and uses the polarity_scores() method of the SentimentIntensityAnalyzer class to calculate the sentiment scores for the tweet text. The sentiment scores are then added to the DataFrame as new columns: sentiment_score, Negative, Neutral, and Positive.\n",
        "\n",
        "The try and except statements are used to handle any TypeError exceptions that may occur when attempting to normalize the tweet text using the unicodedata.normalize() method.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9_rowIsw6n4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "for indx, row in df.T.iteritems():\n",
        "    try:\n",
        "        sentence_i = unicodedata.normalize('NFKD', df.loc[indx, 'Tweet'])\n",
        "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
        "        df.at[indx, 'sentiment_score'] = sentence_sentiment['compound']\n",
        "        df.at[indx, 'Negative'] = sentence_sentiment['neg']\n",
        "        df.at[indx, 'Neutral'] = sentence_sentiment['neu']\n",
        "        df.at[indx, 'Positive'] = sentence_sentiment['pos']\n",
        "    except TypeError:\n",
        "        print (df.loc[indx, 'Tweet'])\n",
        "        print (indx)\n",
        "        break"
      ],
      "metadata": {
        "id": "qxd4LFKYf_oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head_tweets = df.head(12)  # Get the first 5 tweets\n",
        "tail_tweets = df.tail(10)  # Get the last 5 tweets\n",
        "\n",
        "# Display the head tweets with sentiment scores\n",
        "print(\"Head 5 Tweets:\")\n",
        "for index, row in head_tweets.iterrows():\n",
        "    print(\"Tweet:\", row['Tweet'])\n",
        "    print(\"Sentiment Score:\", row['sentiment_score'])\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "# Display the tail tweets with sentiment scores\n",
        "print(\"Tail 5 Tweets:\")\n",
        "for index, row in tail_tweets.iterrows():\n",
        "    print(\"Tweet:\", row['Tweet'])\n",
        "    print(\"Sentiment Score:\", row['sentiment_score'])\n",
        "    print(\"------------------------------\")\n"
      ],
      "metadata": {
        "id": "BcyQO2p-BM5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to group tweets by date and calculate the average sentiment score for each day. It first defines a new DataFrame called \"daily_sentiment\" by grouping the original DataFrame \"df\" by the \"Date\" column and calculating the mean sentiment score for each group using the \"mean()\" method."
      ],
      "metadata": {
        "id": "6BiykpqA78Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the tweets by date and calculate the average sentiment score for each day\n",
        "daily_sentiment = df.groupby('Date')['sentiment_score'].mean().reset_index()\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "mdb_DJ4lgawi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exports the daily sentiment scores to a CSV file named . The resulting CSV file will be saved in the current working directory."
      ],
      "metadata": {
        "id": "PyDYH3qC8I_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_sentiment.to_csv(\"millions_tweets_sentiment_score .csv\")"
      ],
      "metadata": {
        "id": "RIaiPqtUyy3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf62DIom2HJc"
      },
      "outputs": [],
      "source": [
        "print(df['sentiment_score'].mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Date**  column of the DataFrame to a datetime object and drop any rows with missing or invalid date values. The dates are then converted again to a datetime object to ensure consistency, and a new column 'Year' is added to the DataFrame containing the year value of each date."
      ],
      "metadata": {
        "id": "XC7eo1Yr8WJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkSZited4vsa"
      },
      "outputs": [],
      "source": [
        "# Convert 'Date' column to datetime object\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df = df.dropna(subset=['Date'])\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].apply(lambda x: x.year)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='Year', y='sentiment_score', data=df)\n"
      ],
      "metadata": {
        "id": "l2mvinoyl5wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybNwazST2N4K"
      },
      "outputs": [],
      "source": [
        "print(df['sentiment_score'].max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot WordCloud of TSX\n"
      ],
      "metadata": {
        "id": "9uir53tio6yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we scraped millions of tweets related to the **Toronto Stock Exchange (TSX)**, Canadian economy, business, and finance using the snscrape Python library. We then used the Natural Language Toolkit (NLTK) library to perform sentiment analysis on the tweets and analyzed the sentiment trends over time. Additionally, we used the WordCloud library to visualize the most commonly used words in the tweets. The insights gained from this analysis could potentially be useful for investors, analysts, and financial professionals interested in understanding the sentiment and trends surrounding the Canadian financial market.\n",
        "\n"
      ],
      "metadata": {
        "id": "7zFrDT9c9Pbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_vMx7Yj-IuB"
      },
      "outputs": [],
      "source": [
        "# Concatenate all tweets into a single string\n",
        "text = ' '.join(df['Tweet'])\n",
        "\n",
        "# Create WordCloud object\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white',\n",
        "                      min_font_size=10).generate(text)\n",
        "\n",
        "# Plot WordCloud\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate sentiment scores\n"
      ],
      "metadata": {
        "id": "_9iI2KVlsscu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b20amH2P8Xcx"
      },
      "outputs": [],
      "source": [
        "sentiment_scores = df['sentiment_score'].value_counts()\n",
        "\n",
        "# Create bar chart of sentiment scores\n",
        "plt.bar(sentiment_scores.index, sentiment_scores.values)\n",
        "\n",
        "# Set title and axis labels\n",
        "plt.title('Sentiment Analysis of Tweets')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('TSX Tweets ')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZJAm73MznU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# merge the twitter and stock on the date\n"
      ],
      "metadata": {
        "id": "hT1ePg8vF9o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reads in two dataframes (df_twitter and df_stock) from CSV files, converts their 'Date' column to datetime format, and then merges the two dataframes on the 'Date' column to create a new dataframe df_merged.\n",
        "\n"
      ],
      "metadata": {
        "id": "yMAYjVjP9rF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in your dataframes\n",
        "df_twitter = pd.read_csv('/content/millions_tweets_sentiments .csv')\n",
        "df_stock = pd.read_csv('/content/Toronto_stock_data .csv')\n",
        "\n",
        "# convert date column to datetime format for both dataframes\n",
        "df_twitter['Date'] = pd.to_datetime(df_twitter['Date'])\n",
        "df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
        "\n",
        "# merge the two dataframes on the date column\n",
        "df_merged = pd.merge(df_twitter, df_stock, on='Date')"
      ],
      "metadata": {
        "id": "RRDc3VJQ5FXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "-IgSRl7I5lRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.tail()"
      ],
      "metadata": {
        "id": "NlS6E3VnC2DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will remove the Unnamed: 0 column from both dataframes.\n",
        "df_merged = df_merged.drop(['Unnamed: 0'], axis=1)"
      ],
      "metadata": {
        "id": "RQkx_X6fC8gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3 show the merged data frame\n",
        "df_merged.head()"
      ],
      "metadata": {
        "id": "aNKYW5xSDftH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for null values\n",
        "null_values = df_merged.isnull().sum()\n",
        "print(null_values)"
      ],
      "metadata": {
        "id": "4lTaw-W8D_CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the combined data frame\n",
        "df_merged.to_csv('combine_twitter_tsx. csv')"
      ],
      "metadata": {
        "id": "XJuDFzNDED81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "oYiJGOszGq3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains a **convolutional LSTM** neural network on historical stock prices, using data from a CSV file that contains combined data from Twitter sentiment analysis and Toronto Stock Exchange data. The data is preprocessed by splitting it into training and testing sets, scaling the data, and creating sequences of data to feed into the model. The model includes several layers of Conv1D and LSTM layers, and is trained using mean squared error as the loss function. The code then evaluates the model and calculates metrics such as mean squared error, mean absolute error, and R-squared, and plots the predictions and actual values.\n",
        "\n"
      ],
      "metadata": {
        "id": "04xeEHy895-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "df=pd.read_csv(\"/content/combine_twitter_tsx .csv\")\n",
        "# Scrape London Stock Exchange data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Use only the closing prices\n",
        "prices = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "training_size = int(len(prices) * 0.8)\n",
        "train_data = prices[:training_size]\n",
        "test_data = prices[training_size:]\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# Define function to create training and testing sequences\n",
        "def create_sequences(data, time_steps):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(time_steps, len(data)):\n",
        "        X.append(data[i - time_steps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Set time steps and number of features\n",
        "time_steps = 30\n",
        "num_features = 1\n",
        "\n",
        "# Create the train and test sequences\n",
        "X_train, y_train = create_sequences(train_data, time_steps)\n",
        "X_test, y_test = create_sequences(test_data, time_steps)\n",
        "\n",
        "# Reshape the data for the CNN\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], num_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], num_features))\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(time_steps, num_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(500, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(300, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(200, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=500, batch_size=64, verbose=2)\n",
        "\n",
        "# Evaluate model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R2):\", r2)\n",
        "\n",
        "# Invert scaling of predictions and actual values\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "actual_values = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot predictions and actual values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(predictions, label='Predictions')\n",
        "plt.plot(actual_values, label='Actual Values')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HAIKX1PeEmFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actual_values, predictions)\n",
        "mae = mean_absolute_error(actual_values, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(actual_values, predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Mean Squared Error (MSE):', mse)\n",
        "print('Mean Absolute Error (MAE):', mae)\n",
        "print('Root Mean Squared Error (RMSE):', rmse)\n",
        "print('R-squared (R2):', r2)\n"
      ],
      "metadata": {
        "id": "X1zCRRyyHsbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfermor Model"
      ],
      "metadata": {
        "id": "oAXMntZ2G_Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is a TensorFlow implementation of a time series prediction model using a multi-head attention layer. The code loads data from a CSV file, splits it into training and testing sets, scales the data using a MinMaxScaler, and creates training and testing sequences using a function. The model architecture includes an input layer, a normalization layer, a multi-head attention layer, two dropout layers, a dense layer, and an output layer. The model is compiled with the mean squared error loss function and trained using early stopping. The code evaluates the model's performance using mean squared error, mean absolute error, root mean squared error, and r-squared metrics. Finally, the code plots the predicted values against the actual values.\n"
      ],
      "metadata": {
        "id": "uNLCGU8a-Llq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/combine_twitter_tsx .csv\")\n",
        "df.dropna(inplace=True)\n",
        "prices = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "training_size = int(len(prices) * 0.8)\n",
        "train_data = prices[:training_size]\n",
        "test_data = prices[training_size:]\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# Define function to create training and testing sequences\n",
        "def create_sequences(data, time_steps):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(time_steps, len(data)):\n",
        "        X.append(data[i - time_steps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Set time steps and number of features\n",
        "time_steps = 30\n",
        "num_features = 1\n",
        "\n",
        "# Create the train and test sequences\n",
        "X_train, y_train = create_sequences(train_data, time_steps)\n",
        "X_test, y_test = create_sequences(test_data, time_steps)\n",
        "\n",
        "# Define model\n",
        "input_layer = Input(shape=(time_steps, num_features))\n",
        "x = Normalization()(input_layer)\n",
        "x = LayerNormalization()(x)\n",
        "x = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=1)(x, x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "output_layer = Dense(1)(x)\n",
        "model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.2, callbacks=[es])\n",
        "\n",
        "# Evaluate model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R2):\", r2)\n",
        "\n",
        "# Invert scaling of predictions and actual values\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "actual_values = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot predictions and actual values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(predictions, label='Predictions')\n",
        "plt.plot(actual_values, label='Actual Values')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hj46vYcSFdzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transformer Hugging face"
      ],
      "metadata": {
        "id": "kSrIwHc4KEyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "nWilkKjxVuG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is an example of how to create a transformer-based neural network for time-series forecasting using the PyTorch library.\n",
        "\n",
        "The code loads time-series data from a CSV file, splits it into training and testing sets, scales the data, and creates sequences of data with a specified number of time steps. Then, it defines a transformer-based neural network model with one transformer encoder layer, a dropout layer, and two dense layers. The model is compiled using the mean squared error as the loss function and the Adam optimizer.\n",
        "\n",
        "The model is trained on the training data, with early stopping to prevent overfitting. The performance of the model is evaluated on the testing data using several metrics such as mean squared error, mean absolute error, root mean squared error, and R-squared. Finally, the predictions of the model and the actual values are plotted using matplotlib."
      ],
      "metadata": {
        "id": "tjgvCz3r-cfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TransformerEncoder\n",
        "import torch\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/combine_twitter_tsx .csv\")\n",
        "df.dropna(inplace=True)\n",
        "prices = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "training_size = int(len(prices) * 0.8)\n",
        "train_data = prices[:training_size]\n",
        "test_data = prices[training_size:]\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# Define function to create training and testing sequences\n",
        "def create_sequences(data, time_steps):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(time_steps, len(data)):\n",
        "        X.append(data[i - time_steps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Set time steps and number of features\n",
        "time_steps = 30\n",
        "num_features = 1\n",
        "\n",
        "# Create the train and test sequences\n",
        "X_train, y_train = create_sequences(train_data, time_steps)\n",
        "X_test, y_test = create_sequences(test_data, time_steps)\n",
        "\n",
        "# Define model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "transformer_layer = TransformerEncoderLayer(d_model=num_features, nhead=8)\n",
        "transformer_encoder = TransformerEncoder(transformer_layer, num_layers=1)\n",
        "input_layer = layers.Input(shape=(time_steps, num_features))\n",
        "x = Normalization()(input_layer)\n",
        "x = LayerNormalization()(x)\n",
        "x = layers.Lambda(lambda x: x.permute(0, 2, 1))(x)\n",
        "x = layers.Reshape((-1, num_features))(x)\n",
        "x = transformer_encoder(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "output_layer = layers.Dense(1)(x)\n",
        "model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.2, callbacks=[es])\n",
        "\n",
        "# Evaluate model\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R2):\", r2)\n",
        "\n",
        "# Invert scaling of predictions and actual values\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "actual_values = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot predictions and actual values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(predictions, label='Predictions')\n",
        "plt.plot(actual_values, label='Actual Values')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VNFSbyFRLG70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model using the Keras API"
      ],
      "metadata": {
        "id": "b3_LCCzsWbzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer model using the Keras API. This model consists of multiple transformer blocks, each with a specified number of attention heads and feedforward dimensions. The transformer blocks are then flattened and passed through a dense layer to output a single prediction. The model takes as input a sequence of integers with a specified input shape.\n",
        "\n"
      ],
      "metadata": {
        "id": "SnO75Av1-oXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, MultiHeadAttention, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Read data\n",
        "df = pd.read_csv(\"/content/combine_twitter_tsx .csv\")\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Use only the closing prices\n",
        "prices = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "training_size = int(len(prices) * 0.8)\n",
        "train_data = prices[:training_size]\n",
        "test_data = prices[training_size:]\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# Define function to create training and testing sequences\n",
        "def create_sequences(data, time_steps):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(time_steps, len(data)):\n",
        "        X.append(data[i - time_steps:i])\n",
        "        y.append(data[i])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Set time steps\n",
        "time_steps = 30\n",
        "\n",
        "# Create the train and test sequences\n",
        "X_train, y_train = create_sequences(train_data, time_steps)\n",
        "X_test, y_test = create_sequences(test_data, time_steps)\n",
        "\n",
        "# Reshape the data for the LSTM and Transformer models\n",
        "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "X_train_trans = X_train.reshape((X_train.shape[0], X_train.shape[1]))\n",
        "X_test_trans = X_test.reshape((X_test.shape[0], X_test.shape[1]))\n",
        "\n",
        "# Define LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, 1)))\n",
        "model_lstm.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Conv1D(filters=32, kernel_size=3, activation='sigmoid'))\n",
        "model_lstm.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(LSTM(100, return_sequences=True))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(LSTM(50, return_sequences=False))\n",
        "model_lstm.add(Dropout(0.3))\n",
        "model_lstm.add(Dense(50, activation='relu'))\n",
        "model_lstm.add(Dense(1))\n",
        "\n",
        "# Compile LSTM model\n",
        "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Define Transformer model\n",
        "def transformer_model(input_shape):\n",
        "    inputs = Input(shape=(input_shape,))\n",
        "    embedding_layer = Embedding(input_dim=input_shape, output_dim=64)(inputs)\n",
        "    transformer_block = Transformer(num_heads=8, ff_dim=64, dropout=0.2, name='transformer_block_1')(embedding_layer)\n",
        "    transformer_block = Transformer(num_heads=8, ff_dim=64, dropout=0.2, name='transformer_block_2')(transformer_block)\n",
        "    transformer_block = Transformer(num_heads=8, ff_dim=64, dropout=0.2, name='transformer_block_3')(transformer_block)\n",
        "    transformer_block = Flatten()(transformer_block)\n",
        "    outputs = Dense(1)(transformer_block)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='transformer')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "N10aZQZJIfYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_lstm = model_lstm.fit(X_train_lstm, y_train, batch_size=32, epochs=50, validation_data=(X_test_lstm, y_test), callbacks=[EarlyStopping(patience=10)])\n"
      ],
      "metadata": {
        "id": "F0GljNCZK7H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_lstm = model_lstm.evaluate(X_test_lstm, y_test)\n",
        "print('LSTM Test Loss:', score_lstm)\n"
      ],
      "metadata": {
        "id": "32xVEWKYMN2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trans = transformer_model(X_train_trans.shape[1])\n",
        "model_trans.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history_trans = model_trans.fit(X_train_trans, y_train, batch_size=32, epochs=50, validation_data=(X_test_trans, y_test), callbacks=[EarlyStopping(patience=10)])\n",
        "score_trans = model_trans.evaluate(X_test_trans, y_test)\n",
        "print('Transformer Test Loss:', score_trans)\n"
      ],
      "metadata": {
        "id": "DV5CGSxSMV5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Adversarial Network (GAN)"
      ],
      "metadata": {
        "id": "UaygcY6tWjUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generative Adversarial Network (GAN)** to generate synthetic data that resembles real stock market closing prices. The code loads a CSV file containing historical stock market closing prices and preprocesses it by splitting it into training and testing sets and scaling it using MinMaxScaler. It then defines three models: a generator model, a discriminator model, and a GAN model that combines the generator and discriminator models. The generator model takes random noise as input and generates synthetic data that resembles the real stock market closing prices. The discriminator model takes real and synthetic data as input and predicts whether the data is real or fake. The GAN model trains the generator model to produce synthetic data that can fool the discriminator model. The code then trains the GAN model by alternating between training the discriminator model on real and fake data and training the generator model to produce synthetic data that can fool the discriminator model. Finally, it prints the mean squared error **(MSE)**, mean absolute error (**MAE**), root mean squared error **(RMSE)**, and **R2** score between the real and synthetic test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "wodhgLum-5eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/combine_twitter_tsx .csv\")\n",
        "df.dropna(inplace=True)\n",
        "prices = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "training_size = int(len(prices) * 0.8)\n",
        "train_data = prices[:training_size]\n",
        "test_data = prices[training_size:]\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# Set parameters\n",
        "noise_dim = 100\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "# Define generator model\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128, activation='relu', input_dim=noise_dim))\n",
        "generator.add(Dense(256, activation='relu'))\n",
        "generator.add(Dense(512, activation='relu'))\n",
        "generator.add(Dense(train_data.shape[1], activation='sigmoid'))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Define discriminator model\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(512, activation='relu', input_dim=train_data.shape[1]))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(256, activation='relu'))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(128, activation='relu'))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "\n",
        "# Define GAN model\n",
        "discriminator.trainable = False\n",
        "gan_input = Input(shape=(noise_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Define function to create training batches\n",
        "def get_batches(data, batch_size):\n",
        "    batches = []\n",
        "    num_batches = len(data) // batch_size\n",
        "    for i in range(num_batches):\n",
        "        batch = data[i*batch_size:(i+1)*batch_size]\n",
        "        batches.append(batch)\n",
        "    return np.array(batches)\n",
        "\n",
        "# Define function to generate noise samples\n",
        "def generate_noise_samples(num_samples, noise_dim):\n",
        "    return np.random.normal(0, 1, size=[num_samples, noise_dim])\n",
        "\n",
        "# Train GAN model\n",
        "for epoch in range(epochs):\n",
        "    # Train discriminator on real data\n",
        "    real_batches = get_batches(train_data, batch_size)\n",
        "    for batch in real_batches:\n",
        "        noise = generate_noise_samples(batch_size, noise_dim)\n",
        "        fake_data = generator.predict(noise)\n",
        "        discriminator.train_on_batch(batch, np.ones((batch_size, 1)))\n",
        "        discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
        "\n",
        "    # Train generator by fooling discriminator\n",
        "    noise = generate_noise_samples(batch_size, noise_dim)\n",
        "    gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print loss\n",
        "    if epoch % 100 == 0:\n",
        "        noise = generate_noise_samples(len(test_data), noise_dim)\n",
        "        generated_data = generator.predict(noise)\n",
        "        generated_data = scaler.inverse_transform(generated_data)\n",
        "        test_data = scaler.inverse_transform(test_data)\n",
        "        mse = mean_squared_error(test_data, generated_data)\n",
        "        mae = mean_absolute_error(test_data, generated_data)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(test_data, generated_data)\n",
        "        print(\"Epoch:\", epoch, \"- MSE:\", mse, \"- MAE:\", mae, \"- RMSE:\", rmse, \"- R2:\", r2)"
      ],
      "metadata": {
        "id": "rKj65lUAYwvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GAN model\n",
        "for epoch in range(epochs):\n",
        "    # Train discriminator on real data\n",
        "    real_batches = get_batches(train_data, batch_size)\n",
        "    for batch in real_batches:\n",
        "        noise = generate_noise_samples(batch_size, noise_dim)\n",
        "        fake_data = generator.predict(noise)\n",
        "        discriminator.train_on_batch(batch, np.ones((batch_size, 1)))\n",
        "        discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
        "\n",
        "    # Train generator by fooling discriminator\n",
        "    noise = generate_noise_samples(batch_size, noise_dim)\n",
        "    gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print loss\n",
        "    if epoch % 100 == 0:\n",
        "        noise = generate_noise_samples(len(test_data), noise_dim)\n",
        "        generated_data = generator.predict(noise)\n",
        "        generated_data = scaler.inverse_transform(generated_data)\n",
        "        test_data_unscaled = scaler.inverse_transform(test_data)\n",
        "        mse = mean_squared_error(test_data_unscaled, generated_data)\n",
        "        mae = mean_absolute_error(test_data_unscaled, generated_data)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(test_data_unscaled, generated_data)\n",
        "        print(\"Epoch:\", epoch, \"\\tMSE:\", mse, \"\\tMAE:\", mae, \"\\tRMSE:\", rmse, \"\\tR2 Score:\", r2)\n",
        "\n",
        "# Invert scaling of generated data\n",
        "noise = generate_noise_samples(len(test_data), noise_dim)\n",
        "generated_data = generator.predict(noise)\n",
        "generated_data = scaler.inverse_transform(generated_data)\n",
        "\n",
        "# Plot generated data\n",
        "plt.plot(generated_data, label='Generated Data')\n",
        "plt.plot(test_data_unscaled, label='Actual Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ffRUjxyGYyvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.summary()\n",
        "fig.\n"
      ],
      "metadata": {
        "id": "Ka589VXJZdOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.summary()"
      ],
      "metadata": {
        "id": "eIj662JPs_t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('discriminator_summary.txt', 'w') as f:\n",
        "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n'))\n"
      ],
      "metadata": {
        "id": "jQ813IYntCCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('generator_summary.txt', 'w') as f:\n",
        "    generator.summary(print_fn=lambda x: f.write(x + '\\n'))\n"
      ],
      "metadata": {
        "id": "aStd2kq-uiAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.summary()"
      ],
      "metadata": {
        "id": "LTDeEejVuZL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XtEHMy1OtWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('gan_summary.txt', 'w') as f:\n",
        "    gan.summary(print_fn=lambda x: f.write(x + '\\n'))\n"
      ],
      "metadata": {
        "id": "YhC2e1kU3Dkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP6m+NL50j1iddQm1CCyp14",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}